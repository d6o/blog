[{"content":"Reviving My Mom\u0026rsquo;s Love for Retro Gaming with Lakka and a Raspberry Pi 5\nA few months ago, I visited my parents and was inspired to rekindle some fond childhood memories. Back in the day, I had an Atari (not entirely sure which model), a Sega Master System Mark III, and later a PlayStation 1. But what stuck with me the most was how much my mom loved the Sega Master System, particularly Sonic the Hedgehog. She was the only one who could beat the first Sonic game—something I never managed to do as a kid and still haven’t been able to do to this day!\nDuring that visit, I decided to bring back those memories by setting up an emulation system on their Apple TVs. Apple had just started allowing emulation software, and I installed RetroArch, loaded some games, and paired two DualSense controllers. It worked like a charm! I set up two of their Apple TVs, but for some reason, I couldn\u0026rsquo;t get the third one to work before I had to leave.\nA few days later, my mom called to say that the games had disappeared. I was puzzled. RetroArch was still installed, but all the games were gone. After some digging, I discovered that Apple TV doesn\u0026rsquo;t guarantee app storage; it deletes files it deems unnecessary, including my mom\u0026rsquo;s beloved game files. With no way to force the files to stay, I knew I needed a more permanent solution.\nEnter the Raspberry Pi 5 and Lakka Back at my parents\u0026rsquo; house this time, I came prepared. I ordered a Raspberry Pi 5 and started looking for the best way to set it up for emulation. RetroArch had worked well previously, so I started there and soon discovered Lakka, an OS specifically designed for game emulation. It seemed perfect for my needs.\nLakka is a lightweight Linux distribution that turns your Raspberry Pi into a full-blown retro gaming console. Built on RetroArch, it supports a vast number of systems and is incredibly user-friendly. Installation is simple: just flash the Lakka image onto an SD card and upload some ROMs.\nI flashed Lakka onto a 64GB microSD card, installed it on the Raspberry Pi, and started downloading ROMs from Myrient, a trusted source for game files. I even wrote a quick Golang program to automate downloading and extracting ROMs, then enabled SSH on the Raspberry Pi to start uploading the games.\nThe Storage Problem Things were going smoothly until I hit an unexpected issue: \u0026ldquo;no space left on device.\u0026rdquo; I was only uploading Atari ROMs—tiny files of about 3-4 KB each—so I was confused. After SSHing into the Raspberry Pi, I found that the /storage partition had only a few megabytes available, even though I was using a 64GB microSD card.\nThe Lakka installation guide mentioned that the system should expand the file system on the first boot, but it seemed like this hadn\u0026rsquo;t happened. After searching Reddit and StackOverflow, I found some old posts on how to manually resize the partition, but they were outdated and unclear. Then, I turned to the LibRetro Discord channel for help, and within a few minutes, I had an answer. A user named jdalmanza pointed me to a recent post that explained how the latest Lakka release had an issue with SD cards larger than 32GB. The good news: the issue was fixed in the latest nightly release.\nI downloaded the nightly release but also asked if there was a way to manually resize the partition. jdalmanza provided a helpful solution: run the resize2fs command. For me, that meant:\nFinding the device path:\nLakka:~ # df -h Filesystem Size Used Available Use% Mounted on /dev/mmcblk0p2 26.0M 4.8M 20.5M 19% /storage ... Resizing the filesystem:\nresize2fs /dev/mmcblk0p2 This worked like magic! My storage went from 26MB to 55.3GB in seconds:\nLakka:~ # df -h Filesystem Size Used Available Use% Mounted on /dev/mmcblk0p2 55.3G 4.8M 55.3G 0% /storage ... Filling the Pi with Games With the storage issue resolved, I improved my Golang program to download all the ROMs for each system my mom loved: Atari 2600, Sega Master System, and others. In total, I downloaded and uploaded almost 20,000 games via SSH over Wi-Fi, which took some time, but it worked flawlessly.\nHere’s a snapshot of the game systems I set up:\nAtari - 2600 Atari - ST Nintendo - Game Boy Advance Sega - Game Gear Atari - 5200 Commodore - Amiga Nintendo - Game Boy Color Sega - Master System - Mark III Atari - 7800 Microsoft - MSX Nintendo - NES (Headered) Sega - Mega Drive - Genesis Atari - 8-bit Family Nintendo - Game Boy Nintendo - SNES After uploading all the ROMs, I indexed the games in Lakka, which took about 15 minutes. The result? A powerful retro gaming machine with all the games my mom used to love, ready to be played any time she wants.\nConclusion This project was a labor of love, combining nostalgia and tech to create something special for my mom. From overcoming storage issues to automating ROM downloads, it was a rewarding experience. Now, thanks to Lakka and a Raspberry Pi 5, my mom can revisit the glory days of Sonic the Hedgehog and much more.\nThe best part? Seeing her joy when she picked up the controller again, ready to relive those cherished moments.\nIf you\u0026rsquo;re looking to bring retro gaming back into your life or want to share those memories with loved ones, I highly recommend trying Lakka on a Raspberry Pi. It\u0026rsquo;s a fun, rewarding project, and maybe you\u0026rsquo;ll even beat Sonic this time!\n","permalink":"http://localhost:1313/posts/reviving-retro-gaming-with-lakka-raspberry-pi-5/","summary":"Reviving My Mom\u0026rsquo;s Love for Retro Gaming with Lakka and a Raspberry Pi 5\nA few months ago, I visited my parents and was inspired to rekindle some fond childhood memories. Back in the day, I had an Atari (not entirely sure which model), a Sega Master System Mark III, and later a PlayStation 1. But what stuck with me the most was how much my mom loved the Sega Master System, particularly Sonic the Hedgehog.","title":"Reviving Retro Gaming With Lakka Raspberry Pi 5"},{"content":"After switching from Firefox to LibreWolf, I became interested in the idea of self-hosting my own Firefox Sync server. Although I had seen this was possible before, I had never really looked into it—until now. I embarked on a journey to set this up, and while it wasn\u0026rsquo;t completely smooth sailing, I eventually got it working. Here\u0026rsquo;s how it went.\nFinding the Right Sync Server Initial Search: Mozilla\u0026rsquo;s Sync Server Repo I started by searching for \u0026ldquo;firefox sync server github\u0026rdquo; and quickly found Mozilla\u0026rsquo;s syncserver repo. This is an all-in-one package designed for self-hosting a Firefox Sync server. It bundles both the tokenserver for authentication and syncstorage for storage, which sounded like exactly what I needed.\nHowever, there were two red flags:\nThe repository had \u0026ldquo;failed\u0026rdquo; tags in the build history. A warning was prominently displayed stating that the repository was no longer being maintained and pointing to a new project in Rust. Switching to Rust: syncstorage-rs With that in mind, I followed the link to syncstorage-rs, which is a modern, Rust-based version of the original project. It seemed like the more viable option, so I decided to move forward with this one. But first, I wanted to check if there was a ready-to-go Docker image to make deployment easier. Unfortunately, there wasn\u0026rsquo;t one, but the documentation did mention running it with Docker.\nThis is where things started to get complicated.\nDiving Into Docker: Confusion and Complexity Documentation Woes The Docker documentation had some strange parts. For example, it mentioned:\nEnsuring that grpcio and protobuf versions matched the versions used by google-cloud-rust-raw. This sounded odd—shouldn\u0026rsquo;t Docker handle version dependencies automatically? Another confusing part was the instruction to manually copy the contents of mozilla-rust-sdk into the top-level root directory. Again, why wasn\u0026rsquo;t this step automated in the Dockerfile? At this point, I was feeling a bit uneasy but decided to push forward. I reviewed the repo, the Dockerfile, the Makefile, and the circleci workflows. Despite all that, I was still unsure how to proceed.\nA Simpler Solution: syncstorage-rs-docker I then stumbled upon dan-r\u0026rsquo;s syncstorage-rs-docker repo, which had a much simpler Docker setup. The description explained that the author had also encountered issues with the original documentation and decided to create a Docker container for their own infrastructure.\nAt this point, I felt reassured that I wasn\u0026rsquo;t alone in my confusion, and decided to give this setup a try.\nSetting Up the Server: Docker Compose and MariaDB Docker Compose Setup I copied the following services into my docker-compose.yaml:\nfirefox_mariadb: container_name: firefox_mariadb image: linuxserver/mariadb:10.6.13 volumes: - /data/ffsync/dbdata:/config restart: unless-stopped environment: MYSQL_DATABASE: syncstorage MYSQL_USER: sync MYSQL_PASSWORD: syncpass MYSQL_ROOT_PASSWORD: rootpass firefox_syncserver: container_name: firefox_syncserver build: context: /root/ffsync dockerfile: Dockerfile args: BUILDKIT_INLINE_CACHE: 1 restart: unless-stopped ports: - \u0026#34;8000:8000\u0026#34; depends_on: - firefox_mariadb environment: LOGLEVEL: info SYNC_URL: https://mydomain/sync SYNC_CAPACITY: 5 SYNC_MASTER_SECRET: mastersecret METRICS_HASH_SECRET: metricssecret SYNC_SYNCSTORAGE_DATABASE_URL: mysql://sync:usersync@firefox_mariadb:3306/syncstorage_rs SYNC_TOKENSERVER_DATABASE_URL: mysql://sync:usersync@firefox_mariadb:3306/tokenserver_rs A few tips:\nBe cautious with the database passwords. Avoid using special characters like \u0026quot;/|%\u0026quot; as they can cause issues during setup. I optimized the Dockerfile to make better use of caching, which reduced compilation time while testing. Initializing the Database I cloned the repository and copied the Dockerfile and initdb.sh script to my server. After making some tweaks, I ran the following steps to get the database up and running:\nBring up the MariaDB container: docker-compose up -d firefox_mariadb Make the initialization script executable and run it: chmod +x initdb.sh ./initdb.sh Bringing the Stack Online Finally, I brought up the entire stack with:\ndocker-compose up -d Configuring Reverse Proxy with Caddy Next, I needed to update my Caddy reverse proxy to point to the new Sync server. I added the following configuration:\nmydomain:443 { reverse_proxy firefox_syncserver:8000 { } } After updating Caddy with the DNS entry, I restarted the proxy and the sync server was up and running.\nChallenges Faced While I eventually got everything working, there were a few notable challenges along the way:\nDatabase persistence: I had issues with persistent data when restarting the MariaDB container. Make sure to clear out old data if needed. Server storage: My server ran out of space during the build process due to the size of the Docker images and intermediate files. Following the right steps: It took me a while to figure out the right steps, and much of the time was spent experimenting with the Docker setup. Final Thoughts Setting up a self-hosted Firefox Sync server is not the easiest task, especially if you\u0026rsquo;re not very familiar with Docker or database management. The official documentation is confusing, but thanks to community efforts like the syncstorage-rs-docker repo, it’s doable.\nIn the end, it took me about two hours to get everything running, but it was worth it. If you’re looking to control your own Firefox Sync server, this guide should help you avoid some of the pitfalls I encountered.\nHappy syncing!\n","permalink":"http://localhost:1313/posts/firefox-sync-server/","summary":"After switching from Firefox to LibreWolf, I became interested in the idea of self-hosting my own Firefox Sync server. Although I had seen this was possible before, I had never really looked into it—until now. I embarked on a journey to set this up, and while it wasn\u0026rsquo;t completely smooth sailing, I eventually got it working. Here\u0026rsquo;s how it went.\nFinding the Right Sync Server Initial Search: Mozilla\u0026rsquo;s Sync Server Repo I started by searching for \u0026ldquo;firefox sync server github\u0026rdquo; and quickly found Mozilla\u0026rsquo;s syncserver repo.","title":"Self-Hosting a Firefox Sync Server"},{"content":"The internet has been a central part of my life for as long as I can remember. It all started in the early 2000s when my parents bought our first family computer. Back then, internet access was limited, and I could only use it on Saturdays after 2 pm because bandwidth was cheaper during off-peak hours. But those restrictions didn’t bother me; I was hooked on exploring the digital world. I spent countless hours trying to discover new game websites, looking for tips to beat my PlayStation games, or finding fun browser-based games. This was just the beginning.\nThe Early Days: Internet Explorer and Windows 98 Our computer came with Windows 98, and my first window into the internet was Internet Explorer 5. As slow and clunky as it was, it opened up a world of possibilities. One of my greatest early discoveries was learning how to \u0026ldquo;save pages\u0026rdquo; to read offline, a game-changer given our limited internet time. As I became more comfortable with the web, I stumbled across the Brazilian website Baixaki, which was a treasure trove of downloadable software. But since my internet connection was slow, downloads would often take the entire weekend, which only added to the anticipation.\nDiscovering Firefox: A Game Changer While exploring Baixaki, I came across Firefox, and it was like a breath of fresh air. Firefox was faster, more efficient, and for the first time, I felt like I was truly experiencing the potential of the internet. Soon after, I switched to Firefox, making it my default browser and never looking back. Around this time, I even tried a browser with a red dinosaur logo, but its name escapes me now. It wasn’t long before Firefox 1.5 came, followed by Firefox 2, which solidified my loyalty to this browser as I dived deeper into web development and coding.\nEnter Chrome: A Revolution Then came Google Chrome. Its release felt revolutionary. While Firefox had become somewhat resource-heavy and sluggish, Chrome was lightning fast and efficient. Still, early versions of Chrome had some quirks. I remember one in particular—if you closed the browser while downloading something, it would cancel the download without warning. This frustration kept me from switching right away, but I knew that in time, Chrome would resolve these issues. And it did, with fast release cycles that I wasn’t used to in the era of slow software updates.\nAs a noob web developer, part of me hesitated to fully switch due to Firefox’s amazing Web Developer toolbar addon, but over time, Chrome won me over with its speed and simplicity.\nOpera, RockMelt, and Other Adventures Around this period, I also experimented with Opera. It was always highly praised in forums, touted as being faster and more secure than its competitors. While Opera had its moments, there was always something that didn’t quite fit my workflow, and I would inevitably return to Chrome.\nFor a brief but fun period, I switched to RockMelt, a Chromium fork designed for social media interaction. It allowed me to chat and interact with my contacts without having to open social media sites. It was short-lived, but it was a novel concept that reflected how browsers were becoming more than just tools for browsing—they were evolving into comprehensive digital platforms.\nChromium and the Quest for Privacy As my online habits matured, so did my concern for privacy and security. I eventually moved to Ungoogled-Chromium, a more privacy-focused version of Chrome. This was a significant step for me, but as my browsing habits became more plugin-dependent, every attempt to switch back to Firefox failed. Firefox simply didn’t have all the plugins I relied on.\nI stuck with Ungoogled-Chromium for several years, appreciating its minimalist and privacy-centered design. However, life threw a curveball when the company I was working for decided that we could only use \u0026ldquo;verified\u0026rdquo; browsers. That’s when I decided to give Firefox another shot.\nBack to Firefox and the Move to LibreWolf This time, switching back to Firefox was a success. For almost two years, I used Firefox as my main browser, enjoying its flexibility and privacy tools. But as I continued to refine my approach to security and data ownership, I became intrigued by the idea of a browser that was privacy-focused right out of the box. This led me to LibreWolf.\nLibreWolf promised everything I wanted: strong privacy settings, no telemetry, and complete control over my browsing experience. I made the switch recently, and along with it, I set up my own Firefox Sync server, giving me full ownership of my data. It’s an exciting new chapter in my ongoing journey through the world of browsers, and I’m eager to see where this path leads next.\nThe Browser Journey Continues… Looking back, my browser journey has been a fascinating ride through the evolution of the internet itself. From the early days of Internet Explorer and slow, dial-up connections, to Firefox, Chrome, and now LibreWolf, each browser represents a stage of growth in my digital life.\nI doubt this is the last chapter in my story with browsers, but I’m excited about what’s to come. Browsers have come a long way from being simple tools for accessing the web to becoming powerful platforms for privacy, productivity, and creativity. Who knows what the next leap will be? Until then, I’ll continue exploring, learning, and tweaking as I go.\n","permalink":"http://localhost:1313/posts/internet-browsers/","summary":"The internet has been a central part of my life for as long as I can remember. It all started in the early 2000s when my parents bought our first family computer. Back then, internet access was limited, and I could only use it on Saturdays after 2 pm because bandwidth was cheaper during off-peak hours. But those restrictions didn’t bother me; I was hooked on exploring the digital world. I spent countless hours trying to discover new game websites, looking for tips to beat my PlayStation games, or finding fun browser-based games.","title":"My Journey Through Browsers: From Internet Explorer to LibreWolf"},{"content":"In this post, we\u0026rsquo;ll walk through how to use Go to subscribe to new blocks on the Ethereum blockchain and print out the transaction hashes for each block. This is a common task when working with Ethereum, especially for monitoring or analytics purposes.\nPrerequisites Before we start, make sure you have the following:\nGo installed on your machine.\nAccess to an Ethereum node. You can use a service like Infura or run your own Ethereum node.\nThe Go-Ethereum package (go-ethereum) installed. You can install it with:\ngo get github.com/ethereum/go-ethereum Step 1: Setting Up the Project First, create a new Go project and initialize it:\nmkdir eth-subscribe cd eth-subscribe go mod init eth-subscribe Next, we\u0026rsquo;ll need to import the necessary packages. Create a main.go file and add the following imports:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/ethereum/go-ethereum\u0026#34; \u0026#34;github.com/ethereum/go-ethereum/rpc\u0026#34; \u0026#34;github.com/ethereum/go-ethereum/core/types\u0026#34; ) Step 2: Connecting to an Ethereum Node To interact with the Ethereum blockchain, we need to connect to an Ethereum node. For this example, we\u0026rsquo;ll use an Infura endpoint, but you can replace the URL with your own node\u0026rsquo;s address.\nfunc main() { // Replace with your Infura project ID or node address client, err := ethclient.Dial(\u0026#34;wss://mainnet.infura.io/ws/v3/YOUR_INFURA_PROJECT_ID\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to connect to the Ethereum client: %v\u0026#34;, err) } defer client.Close() // Create a new context ctx := context.Background() // Subscribe to new block headers headerCh := make(chan *types.Header) sub, err := client.SubscribeNewHead(ctx, headerCh) if err != nil { log.Fatalf(\u0026#34;Failed to subscribe to new headers: %v\u0026#34;, err) } for { select { case err := \u0026lt;-sub.Err(): log.Fatalf(\u0026#34;Subscription error: %v\u0026#34;, err) case header := \u0026lt;-headerCh: processBlock(header, client) } } } Step 3: Processing the Block Once we have a new block header, we can use it to fetch the block and print out the transaction hashes:\nfunc processBlock(header *types.Header, client *ethclient.Client) { block, err := client.BlockByHash(context.Background(), header.Hash()) if err != nil { log.Printf(\u0026#34;Failed to get block: %v\u0026#34;, err) return } fmt.Printf(\u0026#34;New block #%d with %d transactions\u0026#34;, block.Number().Uint64(), len(block.Transactions())) for _, tx := range block.Transactions() { fmt.Printf(\u0026#34;Transaction hash: %s\u0026#34;, tx.Hash().Hex()) } } Step 4: Running the Program Finally, you can run your program to start subscribing to new blocks and printing out the transaction hashes:\ngo run main.go As new blocks are mined on the Ethereum network, your program will output the block number along with the hashes of all the transactions included in each block.\nConclusion In this tutorial, we have built a simple Go program that subscribes to new blocks on the Ethereum network and prints out the transaction hashes. This is a foundational task for many blockchain applications, such as monitoring transactions, building analytics tools, or creating decentralized applications (dApps).\nFeel free to expand this example by filtering transactions, tracking specific contracts, or integrating with a database to store the block data for further analysis.\n","permalink":"http://localhost:1313/posts/subscribe-ethereum-blocks-print-transaction-hashes/","summary":"In this post, we\u0026rsquo;ll walk through how to use Go to subscribe to new blocks on the Ethereum blockchain and print out the transaction hashes for each block. This is a common task when working with Ethereum, especially for monitoring or analytics purposes.\nPrerequisites Before we start, make sure you have the following:\nGo installed on your machine.\nAccess to an Ethereum node. You can use a service like Infura or run your own Ethereum node.","title":"Using Go to Subscribe to Ethereum New Blocks and Print Transaction Hashes"}]